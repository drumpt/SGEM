{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "# from espnet2.bin.asr_inference import Speech2Text\n",
    "\n",
    "from audio_augmentations import *\n",
    "from speechbrain.lobes.augment import TimeDomainSpecAugment\n",
    "\n",
    "from jiwer import wer\n",
    "\n",
    "from data import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read text: 100%|██████████| 2939/2939 [00:00<00:00, 19945.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]    There are 2939 samples.\n"
     ]
    }
   ],
   "source": [
    "def collect_params(model, train_params, bias_only=False):\n",
    "    params = []\n",
    "    names = []\n",
    "\n",
    "    for np, p in model.named_parameters():\n",
    "        collect = False\n",
    "        if \"all\" in train_params:\n",
    "            collect = True\n",
    "        if 'enc' in train_params and 'enc' in str(np):\n",
    "            collect = True\n",
    "        if 'dec' in train_params and 'dec' in str(np):\n",
    "            collect = True\n",
    "        if 'linear' in train_params and 'fc' in str(np):\n",
    "            collect = True\n",
    "        if 'LN' in train_params and 'norm' in str(np):\n",
    "            collect = True\n",
    "\n",
    "        if collect:\n",
    "            p.requires_grad = True\n",
    "            params.append(p)\n",
    "            names.append(str(np))\n",
    "\n",
    "    return params, names\n",
    "\n",
    "split = [\"test-other\"]\n",
    "# dataset_name = \"chime\"\n",
    "# dataset_dir = \"/home/server08/hdd0/changhun_workspace/CHiME3\"\n",
    "dataset_name = 'librispeech'\n",
    "dataset_dir = '/home/server17/hdd/changhun_workspace/LibriSpeech'\n",
    "\n",
    "batch_size=1\n",
    "extra_noise=0.00\n",
    "steps = 10\n",
    "lr = 2e-5\n",
    "\n",
    "dataset = load_dataset(split, dataset_name, dataset_dir, batch_size, extra_noise)\n",
    "\n",
    "original_model = EncoderDecoderASR.from_hparams(\"speechbrain/asr-crdnn-rnnlm-librispeech\", run_opts={\"device\" : \"cuda\"})\n",
    "model = EncoderDecoderASR.from_hparams(\"speechbrain/asr-crdnn-rnnlm-librispeech\", run_opts={\"device\" : \"cuda\"}).requires_grad_(True)\n",
    "params, _ = collect_params(model, train_params=['all'])\n",
    "optim = torch.optim.Adam(params, lr=lr)\n",
    "mse = nn.MSELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "model_mse = nn.MSELoss()\n",
    "\n",
    "transcriptions_1 = []\n",
    "transcriptions_3 = []\n",
    "transcriptions_5 = []\n",
    "transcriptions_10 = []\n",
    "transcriptions_20 = []\n",
    "transcriptions_40 = []\n",
    "gt_texts = []\n",
    "ori_transcriptions = []\n",
    "durations = []\n",
    "werrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.08\n",
      "0-th adapt WER:  0.08\n",
      "1-th adapt WER:  0.08\n",
      "2-th adapt WER:  0.08\n",
      "3-th adapt WER:  0.08\n",
      "4-th adapt WER:  0.08\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.6\n",
      "0-th adapt WER:  0.6\n",
      "1-th adapt WER:  0.6\n",
      "2-th adapt WER:  0.6\n",
      "3-th adapt WER:  0.6\n",
      "4-th adapt WER:  0.6\n",
      "\n",
      "original WER:  0.10714285714285714\n",
      "0-th adapt WER:  0.10714285714285714\n",
      "1-th adapt WER:  0.10714285714285714\n",
      "2-th adapt WER:  0.10714285714285714\n",
      "3-th adapt WER:  0.10714285714285714\n",
      "4-th adapt WER:  0.10714285714285714\n",
      "\n",
      "original WER:  0.11538461538461539\n",
      "0-th adapt WER:  0.11538461538461539\n",
      "1-th adapt WER:  0.11538461538461539\n",
      "2-th adapt WER:  0.11538461538461539\n",
      "3-th adapt WER:  0.11538461538461539\n",
      "4-th adapt WER:  0.11538461538461539\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.25\n",
      "0-th adapt WER:  0.25\n",
      "1-th adapt WER:  0.25\n",
      "2-th adapt WER:  0.25\n",
      "3-th adapt WER:  0.25\n",
      "4-th adapt WER:  0.25\n",
      "\n",
      "original WER:  0.3\n",
      "0-th adapt WER:  0.3\n",
      "1-th adapt WER:  0.3\n",
      "2-th adapt WER:  0.3\n",
      "3-th adapt WER:  0.3\n",
      "4-th adapt WER:  0.3\n",
      "\n",
      "original WER:  0.1568627450980392\n",
      "0-th adapt WER:  0.1568627450980392\n",
      "1-th adapt WER:  0.1568627450980392\n",
      "2-th adapt WER:  0.1568627450980392\n",
      "3-th adapt WER:  0.1568627450980392\n",
      "4-th adapt WER:  0.1568627450980392\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.3333333333333333\n",
      "0-th adapt WER:  0.3333333333333333\n",
      "1-th adapt WER:  0.3333333333333333\n",
      "2-th adapt WER:  0.3333333333333333\n",
      "3-th adapt WER:  0.3333333333333333\n",
      "4-th adapt WER:  0.3333333333333333\n",
      "\n",
      "original WER:  0.05263157894736842\n",
      "0-th adapt WER:  0.05263157894736842\n",
      "1-th adapt WER:  0.05263157894736842\n",
      "2-th adapt WER:  0.05263157894736842\n",
      "3-th adapt WER:  0.05263157894736842\n",
      "4-th adapt WER:  0.05263157894736842\n",
      "\n",
      "original WER:  0.14285714285714285\n",
      "0-th adapt WER:  0.14285714285714285\n",
      "1-th adapt WER:  0.14285714285714285\n",
      "2-th adapt WER:  0.14285714285714285\n",
      "3-th adapt WER:  0.14285714285714285\n",
      "4-th adapt WER:  0.14285714285714285\n",
      "\n",
      "original WER:  0.10526315789473684\n",
      "0-th adapt WER:  0.10526315789473684\n",
      "1-th adapt WER:  0.10526315789473684\n",
      "2-th adapt WER:  0.10526315789473684\n",
      "3-th adapt WER:  0.10526315789473684\n",
      "4-th adapt WER:  0.10526315789473684\n",
      "\n",
      "original WER:  0.0625\n",
      "0-th adapt WER:  0.0625\n",
      "1-th adapt WER:  0.0625\n",
      "2-th adapt WER:  0.0625\n",
      "3-th adapt WER:  0.0625\n",
      "4-th adapt WER:  0.0625\n",
      "\n",
      "original WER:  0.14285714285714285\n",
      "0-th adapt WER:  0.14285714285714285\n",
      "1-th adapt WER:  0.14285714285714285\n",
      "2-th adapt WER:  0.14285714285714285\n",
      "3-th adapt WER:  0.14285714285714285\n",
      "4-th adapt WER:  0.14285714285714285\n",
      "\n",
      "original WER:  0.06666666666666667\n",
      "0-th adapt WER:  0.06666666666666667\n",
      "1-th adapt WER:  0.06666666666666667\n",
      "2-th adapt WER:  0.06666666666666667\n",
      "3-th adapt WER:  0.06666666666666667\n",
      "4-th adapt WER:  0.06666666666666667\n",
      "\n",
      "original WER:  0.375\n",
      "0-th adapt WER:  0.375\n",
      "1-th adapt WER:  0.375\n",
      "2-th adapt WER:  0.375\n",
      "3-th adapt WER:  0.375\n",
      "4-th adapt WER:  0.375\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.625\n",
      "0-th adapt WER:  0.625\n",
      "1-th adapt WER:  0.625\n",
      "2-th adapt WER:  0.625\n",
      "3-th adapt WER:  0.625\n",
      "4-th adapt WER:  0.625\n",
      "\n",
      "original WER:  0.07692307692307693\n",
      "0-th adapt WER:  0.07692307692307693\n",
      "1-th adapt WER:  0.07692307692307693\n",
      "2-th adapt WER:  0.07692307692307693\n",
      "3-th adapt WER:  0.07692307692307693\n",
      "4-th adapt WER:  0.07692307692307693\n",
      "\n",
      "original WER:  0.3333333333333333\n",
      "0-th adapt WER:  0.3333333333333333\n",
      "1-th adapt WER:  0.3333333333333333\n",
      "2-th adapt WER:  0.3333333333333333\n",
      "3-th adapt WER:  0.3333333333333333\n",
      "4-th adapt WER:  0.3333333333333333\n",
      "\n",
      "original WER:  0.42857142857142855\n",
      "0-th adapt WER:  0.42857142857142855\n",
      "1-th adapt WER:  0.42857142857142855\n",
      "2-th adapt WER:  0.42857142857142855\n",
      "3-th adapt WER:  0.42857142857142855\n",
      "4-th adapt WER:  0.42857142857142855\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.041666666666666664\n",
      "0-th adapt WER:  0.041666666666666664\n",
      "1-th adapt WER:  0.041666666666666664\n",
      "2-th adapt WER:  0.041666666666666664\n",
      "3-th adapt WER:  0.041666666666666664\n",
      "4-th adapt WER:  0.041666666666666664\n",
      "\n",
      "original WER:  0.3\n",
      "0-th adapt WER:  0.3\n",
      "1-th adapt WER:  0.3\n",
      "2-th adapt WER:  0.3\n",
      "3-th adapt WER:  0.3\n",
      "4-th adapt WER:  0.3\n",
      "\n",
      "original WER:  0.06329113924050633\n",
      "0-th adapt WER:  0.06329113924050633\n",
      "1-th adapt WER:  0.06329113924050633\n",
      "2-th adapt WER:  0.06329113924050633\n",
      "3-th adapt WER:  0.06329113924050633\n",
      "4-th adapt WER:  0.06329113924050633\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.07142857142857142\n",
      "0-th adapt WER:  0.07142857142857142\n",
      "1-th adapt WER:  0.07142857142857142\n",
      "2-th adapt WER:  0.07142857142857142\n",
      "3-th adapt WER:  0.07142857142857142\n",
      "4-th adapt WER:  0.07142857142857142\n",
      "\n",
      "original WER:  0.26666666666666666\n",
      "0-th adapt WER:  0.26666666666666666\n",
      "1-th adapt WER:  0.26666666666666666\n",
      "2-th adapt WER:  0.26666666666666666\n",
      "3-th adapt WER:  0.26666666666666666\n",
      "4-th adapt WER:  0.26666666666666666\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.42857142857142855\n",
      "0-th adapt WER:  0.42857142857142855\n",
      "1-th adapt WER:  0.42857142857142855\n",
      "2-th adapt WER:  0.42857142857142855\n",
      "3-th adapt WER:  0.42857142857142855\n",
      "4-th adapt WER:  0.42857142857142855\n",
      "\n",
      "original WER:  0.046153846153846156\n",
      "0-th adapt WER:  0.046153846153846156\n",
      "1-th adapt WER:  0.046153846153846156\n",
      "2-th adapt WER:  0.046153846153846156\n",
      "3-th adapt WER:  0.046153846153846156\n",
      "4-th adapt WER:  0.046153846153846156\n",
      "\n",
      "original WER:  0.047619047619047616\n",
      "0-th adapt WER:  0.047619047619047616\n",
      "1-th adapt WER:  0.047619047619047616\n",
      "2-th adapt WER:  0.047619047619047616\n",
      "3-th adapt WER:  0.047619047619047616\n",
      "4-th adapt WER:  0.047619047619047616\n",
      "\n",
      "original WER:  0.23076923076923078\n",
      "0-th adapt WER:  0.23076923076923078\n",
      "1-th adapt WER:  0.23076923076923078\n",
      "2-th adapt WER:  0.23076923076923078\n",
      "3-th adapt WER:  0.23076923076923078\n",
      "4-th adapt WER:  0.23076923076923078\n",
      "\n",
      "original WER:  0.08695652173913043\n",
      "0-th adapt WER:  0.08695652173913043\n",
      "1-th adapt WER:  0.08695652173913043\n",
      "2-th adapt WER:  0.08695652173913043\n",
      "3-th adapt WER:  0.08695652173913043\n",
      "4-th adapt WER:  0.08695652173913043\n",
      "\n",
      "original WER:  0.14285714285714285\n",
      "0-th adapt WER:  0.14285714285714285\n",
      "1-th adapt WER:  0.14285714285714285\n",
      "2-th adapt WER:  0.14285714285714285\n",
      "3-th adapt WER:  0.14285714285714285\n",
      "4-th adapt WER:  0.14285714285714285\n",
      "\n",
      "original WER:  0.08333333333333333\n",
      "0-th adapt WER:  0.08333333333333333\n",
      "1-th adapt WER:  0.08333333333333333\n",
      "2-th adapt WER:  0.08333333333333333\n",
      "3-th adapt WER:  0.08333333333333333\n",
      "4-th adapt WER:  0.08333333333333333\n",
      "\n",
      "original WER:  0.6\n",
      "0-th adapt WER:  0.6\n",
      "1-th adapt WER:  0.6\n",
      "2-th adapt WER:  0.6\n",
      "3-th adapt WER:  0.6\n",
      "4-th adapt WER:  0.6\n",
      "\n",
      "original WER:  0.125\n",
      "0-th adapt WER:  0.125\n",
      "1-th adapt WER:  0.125\n",
      "2-th adapt WER:  0.125\n",
      "3-th adapt WER:  0.125\n",
      "4-th adapt WER:  0.125\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.13333333333333333\n",
      "0-th adapt WER:  0.13333333333333333\n",
      "1-th adapt WER:  0.13333333333333333\n",
      "2-th adapt WER:  0.13333333333333333\n",
      "3-th adapt WER:  0.13333333333333333\n",
      "4-th adapt WER:  0.13333333333333333\n",
      "\n",
      "original WER:  0.4\n",
      "0-th adapt WER:  0.4\n",
      "1-th adapt WER:  0.4\n",
      "2-th adapt WER:  0.4\n",
      "3-th adapt WER:  0.4\n",
      "4-th adapt WER:  0.4\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.09375\n",
      "0-th adapt WER:  0.09375\n",
      "1-th adapt WER:  0.09375\n",
      "2-th adapt WER:  0.09375\n",
      "3-th adapt WER:  0.09375\n",
      "4-th adapt WER:  0.09375\n",
      "\n",
      "original WER:  0.0625\n",
      "0-th adapt WER:  0.0625\n",
      "1-th adapt WER:  0.0625\n",
      "2-th adapt WER:  0.0625\n",
      "3-th adapt WER:  0.0625\n",
      "4-th adapt WER:  0.0625\n",
      "\n",
      "original WER:  0.25\n",
      "0-th adapt WER:  0.25\n",
      "1-th adapt WER:  0.25\n",
      "2-th adapt WER:  0.25\n",
      "3-th adapt WER:  0.25\n",
      "4-th adapt WER:  0.25\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.047619047619047616\n",
      "0-th adapt WER:  0.047619047619047616\n",
      "1-th adapt WER:  0.047619047619047616\n",
      "2-th adapt WER:  0.047619047619047616\n",
      "3-th adapt WER:  0.047619047619047616\n",
      "4-th adapt WER:  0.047619047619047616\n",
      "\n",
      "original WER:  0.18181818181818182\n",
      "0-th adapt WER:  0.18181818181818182\n",
      "1-th adapt WER:  0.18181818181818182\n",
      "2-th adapt WER:  0.18181818181818182\n",
      "3-th adapt WER:  0.18181818181818182\n",
      "4-th adapt WER:  0.18181818181818182\n",
      "\n",
      "original WER:  0.0967741935483871\n",
      "0-th adapt WER:  0.0967741935483871\n",
      "1-th adapt WER:  0.0967741935483871\n",
      "2-th adapt WER:  0.0967741935483871\n",
      "3-th adapt WER:  0.0967741935483871\n",
      "4-th adapt WER:  0.0967741935483871\n",
      "\n",
      "original WER:  0.05263157894736842\n",
      "0-th adapt WER:  0.05263157894736842\n",
      "1-th adapt WER:  0.05263157894736842\n",
      "2-th adapt WER:  0.05263157894736842\n",
      "3-th adapt WER:  0.05263157894736842\n",
      "4-th adapt WER:  0.05263157894736842\n",
      "\n",
      "original WER:  0.1111111111111111\n",
      "0-th adapt WER:  0.1111111111111111\n",
      "1-th adapt WER:  0.1111111111111111\n",
      "2-th adapt WER:  0.1111111111111111\n",
      "3-th adapt WER:  0.1111111111111111\n",
      "4-th adapt WER:  0.1111111111111111\n",
      "\n",
      "original WER:  0.13333333333333333\n",
      "0-th adapt WER:  0.13333333333333333\n",
      "1-th adapt WER:  0.13333333333333333\n",
      "2-th adapt WER:  0.13333333333333333\n",
      "3-th adapt WER:  0.13333333333333333\n",
      "4-th adapt WER:  0.13333333333333333\n",
      "\n",
      "original WER:  0.19047619047619047\n",
      "0-th adapt WER:  0.19047619047619047\n",
      "1-th adapt WER:  0.19047619047619047\n",
      "2-th adapt WER:  0.19047619047619047\n",
      "3-th adapt WER:  0.19047619047619047\n",
      "4-th adapt WER:  0.19047619047619047\n",
      "\n",
      "original WER:  0.09523809523809523\n",
      "0-th adapt WER:  0.09523809523809523\n",
      "1-th adapt WER:  0.09523809523809523\n",
      "2-th adapt WER:  0.09523809523809523\n",
      "3-th adapt WER:  0.09523809523809523\n",
      "4-th adapt WER:  0.09523809523809523\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  1.34375\n",
      "0-th adapt WER:  1.34375\n",
      "1-th adapt WER:  1.34375\n",
      "2-th adapt WER:  1.34375\n",
      "3-th adapt WER:  1.34375\n",
      "4-th adapt WER:  1.34375\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.058823529411764705\n",
      "0-th adapt WER:  0.058823529411764705\n",
      "1-th adapt WER:  0.058823529411764705\n",
      "2-th adapt WER:  0.058823529411764705\n",
      "3-th adapt WER:  0.058823529411764705\n",
      "4-th adapt WER:  0.058823529411764705\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.14285714285714285\n",
      "0-th adapt WER:  0.14285714285714285\n",
      "1-th adapt WER:  0.14285714285714285\n",
      "2-th adapt WER:  0.14285714285714285\n",
      "3-th adapt WER:  0.14285714285714285\n",
      "4-th adapt WER:  0.14285714285714285\n",
      "\n",
      "original WER:  0.05263157894736842\n",
      "0-th adapt WER:  0.05263157894736842\n",
      "1-th adapt WER:  0.05263157894736842\n",
      "2-th adapt WER:  0.05263157894736842\n",
      "3-th adapt WER:  0.05263157894736842\n",
      "4-th adapt WER:  0.05263157894736842\n",
      "\n",
      "original WER:  0.5714285714285714\n",
      "0-th adapt WER:  0.5714285714285714\n",
      "1-th adapt WER:  0.5714285714285714\n",
      "2-th adapt WER:  0.5714285714285714\n",
      "3-th adapt WER:  0.5714285714285714\n",
      "4-th adapt WER:  0.5714285714285714\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.2857142857142857\n",
      "0-th adapt WER:  0.2857142857142857\n",
      "1-th adapt WER:  0.2857142857142857\n",
      "2-th adapt WER:  0.2857142857142857\n",
      "3-th adapt WER:  0.2857142857142857\n",
      "4-th adapt WER:  0.2857142857142857\n",
      "\n",
      "original WER:  0.15384615384615385\n",
      "0-th adapt WER:  0.15384615384615385\n",
      "1-th adapt WER:  0.15384615384615385\n",
      "2-th adapt WER:  0.15384615384615385\n",
      "3-th adapt WER:  0.15384615384615385\n",
      "4-th adapt WER:  0.15384615384615385\n",
      "\n",
      "original WER:  0.3333333333333333\n",
      "0-th adapt WER:  0.3333333333333333\n",
      "1-th adapt WER:  0.3333333333333333\n",
      "2-th adapt WER:  0.3333333333333333\n",
      "3-th adapt WER:  0.3333333333333333\n",
      "4-th adapt WER:  0.3333333333333333\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.0\n",
      "0-th adapt WER:  0.0\n",
      "1-th adapt WER:  0.0\n",
      "2-th adapt WER:  0.0\n",
      "3-th adapt WER:  0.0\n",
      "4-th adapt WER:  0.0\n",
      "\n",
      "original WER:  0.11764705882352941\n",
      "0-th adapt WER:  0.11764705882352941\n",
      "1-th adapt WER:  0.11764705882352941\n",
      "2-th adapt WER:  0.11764705882352941\n",
      "3-th adapt WER:  0.11764705882352941\n",
      "4-th adapt WER:  0.11764705882352941\n",
      "\n",
      "original WER:  0.0975609756097561\n",
      "0-th adapt WER:  0.0975609756097561\n",
      "1-th adapt WER:  0.0975609756097561\n",
      "2-th adapt WER:  0.0975609756097561\n",
      "3-th adapt WER:  0.0975609756097561\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 232.00 MiB (GPU 0; 11.91 GiB total capacity; 8.77 GiB already allocated; 185.00 MiB free; 11.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11381/600517424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0madapt_transcription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0madapt_wer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_transcription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/speechbrain/pretrained/interfaces.py\u001b[0m in \u001b[0;36mtranscribe_batch\u001b[0;34m(self, wavs, wav_lens)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mwav_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0mencoder_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m             \u001b[0mpredicted_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m             predicted_words = [\n\u001b[1;32m    589\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/speechbrain/decoders/seq2seq.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, enc_states, wav_len)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             log_probs, memory, attn = self.forward_step(\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0minp_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_lens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             )\n\u001b[1;32m    631\u001b[0m             \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/speechbrain/decoders/seq2seq.py\u001b[0m in \u001b[0;36mforward_step\u001b[0;34m(self, inp_tokens, memory, enc_states, enc_lens)\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m             dec_out, hs, c, w = self.dec.forward_step(\n\u001b[0;32m--> 944\u001b[0;31m                 \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_lens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m             )\n\u001b[1;32m    946\u001b[0m             \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/speechbrain/nnet/RNN.py\u001b[0m in \u001b[0;36mforward_step\u001b[0;34m(self, inp, hs, c, enc_states, enc_len)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mcell_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m         \u001b[0mdec_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0mdec_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/speechbrain/nnet/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, enc_states, enc_len, dec_states)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mdec_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         attn = self.mlp_attn(\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecomputed_enc_h\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdec_h\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         ).squeeze(-1)\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 232.00 MiB (GPU 0; 11.91 GiB total capacity; 8.77 GiB already allocated; 185.00 MiB free; 11.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for batch in dataset:\n",
    "    lens, wavs, texts, files = batch\n",
    "    wavs = torch.tensor(wavs)\n",
    "\n",
    "    model.eval()\n",
    "    noise = (0.01 * torch.randn_like(wavs)).requires_grad_(True)\n",
    "    with torch.no_grad():\n",
    "        ori_transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.ones(len(wavs)))\n",
    "    ori_wer = wer(list(texts), list(ori_transcription))\n",
    "    print(\"\\noriginal WER: \", ori_wer)\n",
    "\n",
    "    for step_idx in range(5):\n",
    "        model.train()\n",
    "        clean_enc_outputs = model.encode_batch(wavs, wav_lens=torch.ones(len(wavs)))\n",
    "        noisy_enc_outputs = model.encode_batch(wavs + noise, wav_lens=torch.ones(len(wavs)))\n",
    "\n",
    "        noise.grad = torch.zeros_like(noise)\n",
    "        loss = mse(clean_enc_outputs.detach(), noisy_enc_outputs)\n",
    "        loss.backward(retain_graph=True)\n",
    "        noise = noise + 0.3 * noise.grad\n",
    "\n",
    "        clean_enc_outputs = model.encode_batch(wavs, wav_lens=torch.ones(len(wavs)))\n",
    "        ada_noisy_enc_outputs = model.encode_batch(wavs + noise, wav_lens=torch.ones(len(wavs)))\n",
    "        model_loss = model_mse(clean_enc_outputs.detach(), ada_noisy_enc_outputs)\n",
    "        optim.zero_grad()\n",
    "        model_loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        model.eval()\n",
    "        adapt_transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.ones(len(wavs)))\n",
    "\n",
    "        adapt_wer = wer(list(texts), list(ori_transcription))\n",
    "        print(f\"{step_idx}-th adapt WER: \", adapt_wer)\n",
    "\n",
    "    # ada_noisy_transcription, _ = model.transcribe_batch(wavs + noise, wav_lens=torch.ones(len(wavs)))\n",
    "    # ori_wer = wer(list(texts), list(ada_noisy_transcription))\n",
    "    # print(f\"adapt noisy WER: {ori_wer}\")\n",
    "\n",
    "    # print(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "    # model_loss = mse(clean_enc_outputs, ada_noisy_enc_outputs)\n",
    "\n",
    "    # optim.zero_grad()\n",
    "    # model_loss.backward()\n",
    "    # optim.step()\n",
    "\n",
    "    # for i in range(steps):\n",
    "    #     model.train()\n",
    "    #     weak_wavs = weak_augmentation(wavs.detach().cpu())\n",
    "    #     original_rep = model.encode_batch(wavs, wav_lens=torch.tensor([1.0]))\n",
    "    #     weak_rep = model.encode_batch(weak_wavs, wav_lens=torch.tensor([1.0]))\n",
    "\n",
    "    #     loss = mse(weak_rep, original_rep.detach())\n",
    "    #     optim.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     optim.step()\n",
    "\n",
    "    #     if i == 0:\n",
    "    #         model.eval()\n",
    "    #         transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.tensor([1.0]))\n",
    "    #         ada_wer = wer(list(texts), list(transcription))\n",
    "    #         print(\"adapt-1 WER: \", ada_wer)\n",
    "    #         transcriptions_1 += transcription\n",
    "        \n",
    "    #     if i == 2:\n",
    "    #         model.eval()\n",
    "    #         transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.tensor([1.0]))\n",
    "    #         ada_wer = wer(list(texts), list(transcription))\n",
    "    #         print(\"adapt-3 WER: \", ada_wer)\n",
    "    #         transcriptions_3 += transcription\n",
    "        \n",
    "    #     if i == 4:\n",
    "    #         model.eval()\n",
    "    #         transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.tensor([1.0]))\n",
    "    #         ada_wer = wer(list(texts), list(transcription))\n",
    "    #         print(\"adapt-5 WER: \", ada_wer)\n",
    "    #         transcriptions_5 += transcription\n",
    "\n",
    "    #     if i == 9:\n",
    "    #         model.eval()\n",
    "    #         transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.tensor([1.0]))\n",
    "    #         ada_wer = wer(list(texts), list(transcription))\n",
    "    #         print(\"adapt-10 WER: \", ada_wer)\n",
    "    #         transcriptions_10 += transcription\n",
    "\n",
    "# print(\"original WER:\", wer(gt_texts, ori_transcriptions))\n",
    "# if steps >= 10: \n",
    "#     print(\"TTA-1 WER:\", wer(gt_texts, transcriptions_1))\n",
    "#     print(\"TTA-3 WER:\", wer(gt_texts, transcriptions_3))\n",
    "#     print(\"TTA-5 WER:\", wer(gt_texts, transcriptions_5))\n",
    "#     print(\"TTA-10 WER:\", wer(gt_texts, transcriptions_10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('mfa': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b4b963908d25527fd55cf5357b98a3b37541893550afc161b156d00f4db24dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
