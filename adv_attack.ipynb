{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "# from espnet2.bin.asr_inference import Speech2Text\n",
    "\n",
    "from audio_augmentations import *\n",
    "from speechbrain.lobes.augment import TimeDomainSpecAugment\n",
    "\n",
    "from jiwer import wer\n",
    "\n",
    "from data import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_params(model, train_params, bias_only=False):\n",
    "    params = []\n",
    "    names = []\n",
    "\n",
    "    for np, p in model.named_parameters():\n",
    "        collect = False\n",
    "        if \"all\" in train_params:\n",
    "            collect = True\n",
    "        if 'enc' in train_params and 'enc' in str(np):\n",
    "            collect = True\n",
    "        if 'dec' in train_params and 'dec' in str(np):\n",
    "            collect = True\n",
    "        if 'linear' in train_params and 'fc' in str(np):\n",
    "            collect = True\n",
    "        if 'LN' in train_params and 'norm' in str(np):\n",
    "            collect = True\n",
    "\n",
    "        if collect:\n",
    "            p.requires_grad = True\n",
    "            params.append(p)\n",
    "            names.append(str(np))\n",
    "\n",
    "    return params, names\n",
    "\n",
    "split = [\"test-other\"]\n",
    "# dataset_name = \"chime\"\n",
    "# dataset_dir = \"/home/server08/hdd0/changhun_workspace/CHiME3\"\n",
    "dataset_name = 'librispeech'\n",
    "dataset_dir = '/home/server17/hdd/changhun_workspace/LibriSpeech'\n",
    "\n",
    "batch_size=1\n",
    "extra_noise=0.00\n",
    "steps = 10\n",
    "lr = 2e-5\n",
    "\n",
    "dataset = load_dataset(split, dataset_name, dataset_dir, batch_size, extra_noise)\n",
    "\n",
    "original_model = EncoderDecoderASR.from_hparams(\"speechbrain/asr-crdnn-rnnlm-librispeech\", run_opts={\"device\" : \"cuda\"})\n",
    "model = EncoderDecoderASR.from_hparams(\"speechbrain/asr-crdnn-rnnlm-librispeech\", run_opts={\"device\" : \"cuda\"}).requires_grad_(True)\n",
    "params, _ = collect_params(model, train_params=['all'])\n",
    "optim = torch.optim.Adam(params, lr=lr)\n",
    "mse = nn.MSELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "model_mse = nn.MSELoss()\n",
    "\n",
    "transcriptions_1 = []\n",
    "transcriptions_3 = []\n",
    "transcriptions_5 = []\n",
    "transcriptions_10 = []\n",
    "transcriptions_20 = []\n",
    "transcriptions_40 = []\n",
    "gt_texts = []\n",
    "ori_transcriptions = []\n",
    "durations = []\n",
    "werrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset:\n",
    "    lens, wavs, texts, files = batch\n",
    "    wavs = torch.tensor(wavs)\n",
    "\n",
    "    model.eval()\n",
    "    noise = (0.01 * torch.randn_like(wavs)).requires_grad_(True)\n",
    "    with torch.no_grad():\n",
    "        ori_transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.ones(len(wavs)))\n",
    "    ori_wer = wer(list(texts), list(ori_transcription))\n",
    "    print(\"\\noriginal WER: \", ori_wer)\n",
    "\n",
    "    for step_idx in range(5):\n",
    "        model.train()\n",
    "        clean_enc_outputs = model.encode_batch(wavs, wav_lens=torch.ones(len(wavs)))\n",
    "        noisy_enc_outputs = model.encode_batch(wavs + noise, wav_lens=torch.ones(len(wavs)))\n",
    "\n",
    "        noise.grad = torch.zeros_like(noise)\n",
    "        loss = mse(clean_enc_outputs.detach(), noisy_enc_outputs)\n",
    "        loss.backward(retain_graph=True)\n",
    "        noise = noise + 0.3 * noise.grad\n",
    "\n",
    "        clean_enc_outputs = model.encode_batch(wavs, wav_lens=torch.ones(len(wavs)))\n",
    "        ada_noisy_enc_outputs = model.encode_batch(wavs + noise, wav_lens=torch.ones(len(wavs)))\n",
    "        model_loss = model_mse(clean_enc_outputs.detach(), ada_noisy_enc_outputs)\n",
    "        optim.zero_grad()\n",
    "        model_loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        model.eval()\n",
    "        adapt_transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.ones(len(wavs)))\n",
    "\n",
    "        adapt_wer = wer(list(texts), list(ori_transcription))\n",
    "        print(f\"{step_idx}-th adapt WER: \", adapt_wer)\n",
    "\n",
    "    # ada_noisy_transcription, _ = model.transcribe_batch(wavs + noise, wav_lens=torch.ones(len(wavs)))\n",
    "    # ori_wer = wer(list(texts), list(ada_noisy_transcription))\n",
    "    # print(f\"adapt noisy WER: {ori_wer}\")\n",
    "\n",
    "    # print(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "    # model_loss = mse(clean_enc_outputs, ada_noisy_enc_outputs)\n",
    "\n",
    "    # optim.zero_grad()\n",
    "    # model_loss.backward()\n",
    "    # optim.step()\n",
    "\n",
    "    # for i in range(steps):\n",
    "    #     model.train()\n",
    "    #     weak_wavs = weak_augmentation(wavs.detach().cpu())\n",
    "    #     original_rep = model.encode_batch(wavs, wav_lens=torch.tensor([1.0]))\n",
    "    #     weak_rep = model.encode_batch(weak_wavs, wav_lens=torch.tensor([1.0]))\n",
    "\n",
    "    #     loss = mse(weak_rep, original_rep.detach())\n",
    "    #     optim.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     optim.step()\n",
    "\n",
    "    #     if i == 0:\n",
    "    #         model.eval()\n",
    "    #         transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.tensor([1.0]))\n",
    "    #         ada_wer = wer(list(texts), list(transcription))\n",
    "    #         print(\"adapt-1 WER: \", ada_wer)\n",
    "    #         transcriptions_1 += transcription\n",
    "        \n",
    "    #     if i == 2:\n",
    "    #         model.eval()\n",
    "    #         transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.tensor([1.0]))\n",
    "    #         ada_wer = wer(list(texts), list(transcription))\n",
    "    #         print(\"adapt-3 WER: \", ada_wer)\n",
    "    #         transcriptions_3 += transcription\n",
    "        \n",
    "    #     if i == 4:\n",
    "    #         model.eval()\n",
    "    #         transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.tensor([1.0]))\n",
    "    #         ada_wer = wer(list(texts), list(transcription))\n",
    "    #         print(\"adapt-5 WER: \", ada_wer)\n",
    "    #         transcriptions_5 += transcription\n",
    "\n",
    "    #     if i == 9:\n",
    "    #         model.eval()\n",
    "    #         transcription, _ = model.transcribe_batch(wavs, wav_lens=torch.tensor([1.0]))\n",
    "    #         ada_wer = wer(list(texts), list(transcription))\n",
    "    #         print(\"adapt-10 WER: \", ada_wer)\n",
    "    #         transcriptions_10 += transcription\n",
    "\n",
    "# print(\"original WER:\", wer(gt_texts, ori_transcriptions))\n",
    "# if steps >= 10: \n",
    "#     print(\"TTA-1 WER:\", wer(gt_texts, transcriptions_1))\n",
    "#     print(\"TTA-3 WER:\", wer(gt_texts, transcriptions_3))\n",
    "#     print(\"TTA-5 WER:\", wer(gt_texts, transcriptions_5))\n",
    "#     print(\"TTA-10 WER:\", wer(gt_texts, transcriptions_10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 14:32:16) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68f94fffec405b2866f196244cddb945cc3c2bd56280d1f16accca02944a19bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
